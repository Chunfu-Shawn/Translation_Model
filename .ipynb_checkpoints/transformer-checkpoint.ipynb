{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a053f8f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nABBR.\\nbs: batch size,\\nseq_len: max src/trg token-sequence length,\\ndk: key/value size; head dimensionality\\nheads/h: number of heads\\nd_model: model dimension\\npe: positional encoding\\ndff:  inner-layer dimensionality\\np_drop: probability of dropout\\nffn:  position-wise feed-forward networks\\nMHA: multi-head attention\\n'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from data_generate_RPF_count import *\n",
    "from dataset_prepare import *\n",
    "\n",
    "__author__ = \"Chunfu Xiao\"\n",
    "__contributor__=\"...\"\n",
    "__copyright__ = \"\"\n",
    "__credits__ = []\n",
    "__license__ = \"\"\n",
    "__version__=\"1.0.0\"\n",
    "__maintainer__ = \"Chunfu Xiao\"\n",
    "__email__ = \"chunfushawn@gmail.com\"\n",
    "\n",
    "\"\"\"\n",
    "ABBR.\n",
    "bs: batch size,\n",
    "seq_len: max src/trg token-sequence length,\n",
    "dk: key/value size; head dimensionality\n",
    "heads/h: number of heads\n",
    "d_model: model dimension\n",
    "pe: positional encoding\n",
    "dff:  inner-layer dimensionality\n",
    "p_drop: probability of dropout\n",
    "ffn:  position-wise feed-forward networks\n",
    "MHA: multi-head attention\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c99f4b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replicate_module(module, copies):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(copies)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4de954c",
   "metadata": {},
   "source": [
    "\n",
    "# Part1: =============== modules ================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "944a3749",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(input_dim, output_dim)\n",
    "        self.key = nn.Linear(input_dim, output_dim)\n",
    "        self.value = nn.Linear(input_dim, output_dim)\n",
    "        self.dk = output_dim\n",
    "\n",
    "    # Scaled dot-product attention:\n",
    "    def self_attention(self, query, key, value, mask):\n",
    "        # query/key/value:  (bs,  seq_len, dk)/(bs, heads, seq_len, dk)\n",
    "        # mask shape = (bs, 1, seq_len)/(bs, 1, 1, seq_len)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.dk)) # (bs, seq_len, seq_len)/(bs, heads, seq_len, seq_len)\n",
    "        if mask is not None:\n",
    "            scores.masked_fill_(mask == torch.tensor(False), float(\"-inf\"))\n",
    "        # Softmax dim=-1 stands for apply the softmax along the last dimension\n",
    "        attention_weights = nn.Softmax(dim=-1)(scores)  # (bs, heads, seq_len, seq_len)/(bs, seq_len, seq_len)\n",
    "        attention_qkv = torch.matmul(attention_weights, value)   # (bs, seq_len, dk)/(bs, heads, seq_len, dk)\n",
    "        return attention_qkv\n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        # qkv shape: (bs, seq_len, d_model)\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)\n",
    "        value = self.value(value)\n",
    "        attention_qkv = self.self_attention(query, key, value, mask)  # shape:  (bs, seq_len, d_model)\n",
    "        return attention_qkv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3e8a99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(Attention):\n",
    "    def __init__(self, d_model, heads):\n",
    "        super().__init__(d_model, d_model)\n",
    "        assert d_model % heads == 0\n",
    "        self.dk = d_model // heads  # head dimension\n",
    "        self.heads = heads\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "        self.sqrt_dk = torch.sqrt(torch.tensor(self.dk))\n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        batch_size = query.shape[0]\n",
    "        # qkv shape: (bs, seq_len, dk*heads)\n",
    "        # dk * heads = d_model\n",
    "        query = self.query(query).view(batch_size, -1, self.heads, self.dk).transpose(1, 2)\n",
    "        key = self.key(key).view(batch_size, -1, self.heads, self.dk).transpose(1, 2)\n",
    "        value = self.value(value).view(batch_size, -1, self.heads, self.dk).transpose(1, 2)\n",
    "        attention_qkv = self.self_attention(query, key, value, mask)  # shape:  (bs, heads, seq_len, dk)\n",
    "        #  (bs, heads, seq_len, dk) -> (bs, seq_len, dk*heads)\n",
    "        reshaped = attention_qkv.transpose(1, 2).reshape(batch_size, -1, self.heads * self.dk)\n",
    "        representations_batch = self.out_linear(reshaped)\n",
    "        return representations_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7f3d841",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttentionV2(nn.Module):\n",
    "    \"\"\" Write self_attention into MHA \"\"\"\n",
    "    def __init__(self, d_model, heads):\n",
    "        super().__init__()\n",
    "        assert d_model % heads == 0\n",
    "        self.dk = d_model // heads  # head dimension\n",
    "        self.heads = heads\n",
    "        self.qkv_nets = (nn.Linear(d_model, d_model), nn.Linear(d_model, d_model), nn.Linear(d_model, d_model))\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "        self.sqrt_dk = torch.sqrt(torch.tensor(self.dk))\n",
    "\n",
    "    # Scaled dot-product attention:\n",
    "    def attention(self, query, key, value, mask):\n",
    "        # query/key/value shape (bs, heads, seq_len, dk)\n",
    "        # mask shape = (bs, 1, 1, seq_len) or (bs, 1, seq_len, seq_len)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / self.sqrt_dk  # shape: (bs, heads, seq_len, seq_len)\n",
    "        if mask is not None:\n",
    "            scores.masked_fill_(mask == torch.tensor(False), float(\"-inf\"))\n",
    "        # Softmax dim=-1 stands for apply the softmax along the last dimension\n",
    "        attention_weights = nn.Softmax(dim=-1)(scores)  # shape: (bs, heads, seq_len, seq_len)\n",
    "        attention_qkv = torch.matmul(attention_weights, value)   # shape:  (bs, heads, seq_len, dk)\n",
    "        return attention_qkv\n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        batch_size = query.shape[0]\n",
    "        # qkv shape: (bs, seq_len, dk*heads)\n",
    "        # dk * heads = d_model\n",
    "        query, key, value = [net(x).view(batch_size, -1, self.heads, self.dk).transpose(1, 2)\n",
    "                             for net, x in zip(self.qkv_nets, (query, key, value))]\n",
    "        attention_qkv = self.attention(query, key, value, mask)  # shape:  (bs, heads, seq_len, dk)\n",
    "        #  (bs, heads, seq_len, dk) -> (bs, seq_len, dk*heads)\n",
    "        reshaped = attention_qkv.transpose(1, 2).reshape(batch_size, -1, self.heads * self.dk)\n",
    "        representations_batch = self.out_linear(reshaped)\n",
    "        return representations_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "304e5fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, p_drop=None, max_seq_length=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=p_drop) if p_drop is not None else None\n",
    "        position_id = torch.arange(0, max_seq_length).unsqueeze(1)  # (max_seq_length, 1)\n",
    "        frequencies = torch.pow(10000., -torch.arange(0, d_model, 2, dtype=torch.float) / d_model)\n",
    "\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position_id * frequencies)  # sine on even positions\n",
    "        pe[:, 1::2] = torch.cos(position_id * frequencies)  # cosine on odd positions\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, embeddings_batch):\n",
    "        ''' return vocabulary embedings plus position information'''\n",
    "        # embedding_batch  shape: (bs, seq_len, d_model)\n",
    "        # pe shape: (max_seq_length, d_model)\n",
    "        # pe shape broad_casted -> (bs, seq_len, d_model)\n",
    "        assert embeddings_batch.ndim == 3 and embeddings_batch.shape[-1] == self.pe.shape[-1]\n",
    "        # add position information into vocabulary embedding\n",
    "        positional_encodings = embeddings_batch + self.pe[:embeddings_batch.shape[1]]\n",
    "        if self.dropout is not None:\n",
    "            positional_encodings = self.dropout(positional_encodings)\n",
    "        return positional_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "264df12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embeddings_layer = nn.Embedding(vocab_size, d_model)\n",
    "        self.sqrt_d_model = torch.sqrt(torch.tensor(d_model))\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        assert tokens.ndim == 2\n",
    "        # tokens shape: (bs, seq_len)\n",
    "        # embeddings shape: (bs, seq_len, d_model), every token id has associated vector\n",
    "        embeddings = self.embeddings_layer(tokens)\n",
    "        # Paper P-5, Chapter 3.4 \"Embeddings and Softmax\": multiply the embedding weights by the square root of d_model\n",
    "        embeddings = embeddings * self.sqrt_d_model\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0864e7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, dff=2048):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, dff)\n",
    "        self.linear2 = nn.Linear(dff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, representations_batch):\n",
    "        return self.linear2(self.relu(self.linear1(representations_batch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e59b2fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNormLayer(nn.Module):\n",
    "    def __init__(self, d_model, p_prob):\n",
    "        super().__init__()\n",
    "        self.LN = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(p=p_prob)\n",
    "\n",
    "    def forward(self, representations_batch, sublayer_module):\n",
    "        return representations_batch + self.dropout(sublayer_module(self.LN(representations_batch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b81c4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, heads, p_prob,):\n",
    "        super().__init__()\n",
    "        self.sublayers = replicate_module(AddNormLayer(d_model, p_prob), 2)\n",
    "        self.multi_headed_attention = MultiHeadedAttention(d_model, heads)\n",
    "        self.ffn = PositionwiseFeedForward(d_model)\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, src_representations_batch, src_mask):\n",
    "        # Define anonymous (lambda) function which only takes src_representations_batch (srb) as input,\n",
    "        # this way we have a uniform interface for the sublayer logic.\n",
    "        encoder_self_attention = lambda srb: self.multi_headed_attention(query=srb, key=srb, value=srb, mask=src_mask)\n",
    "\n",
    "        # Self-attention MHA sublayer followed by point-wise feed forward net sublayer\n",
    "        src_representations_batch = self.sublayers[0](src_representations_batch, encoder_self_attention)\n",
    "        src_representations_batch = self.sublayers[1](src_representations_batch, self.ffn)\n",
    "\n",
    "        return src_representations_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f55622d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, heads, p_prob):\n",
    "        super().__init__()\n",
    "        self.sublayers = replicate_module(AddNormLayer(d_model, p_prob), 3)\n",
    "        self.trg_multi_headed_attention = MultiHeadedAttention(d_model, heads)\n",
    "        self.src_multi_headed_attention = MultiHeadedAttention(d_model, heads)\n",
    "        self.ffn = PositionwiseFeedForward(d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, trg_representations_batch, src_representations_batch, trg_mask, src_mask):\n",
    "        srb = src_representations_batch\n",
    "        decoder_trg_self_attention = lambda trb: self.trg_multi_headed_attention(query=trb, key=trb, value=trb, mask=trg_mask)\n",
    "        decoder_src_attention = lambda trb: self.src_multi_headed_attention(query=trb, key=srb, value=srb, mask=src_mask)\n",
    "\n",
    "        # Self-attention MHA sublayer followed by a source-attending MHA and point-wise feed forward net sublayer\n",
    "        trg_representations_batch = self.sublayers[0](trg_representations_batch, decoder_trg_self_attention)\n",
    "        trg_representations_batch = self.sublayers[1](trg_representations_batch, decoder_src_attention)\n",
    "        trg_representations_batch = self.sublayers[2](trg_representations_batch, self.ffn)\n",
    "\n",
    "        return trg_representations_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a71bc696",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, trg_representations_batch):\n",
    "        # trg_representations_batch shape: (bs, seq_len, d_model)\n",
    "        # output shape: (bs, seq_len, vocab_size)\n",
    "        return self.log_softmax(self.linear(trg_representations_batch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14a7ee4",
   "metadata": {},
   "source": [
    "# Part2: =========== Encoder&Decoder =============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fdfd04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, number_of_layers):\n",
    "        super().__init__()\n",
    "        self.encoder_layers = replicate_module(encoder_layer, number_of_layers)\n",
    "        self.LN = nn.LayerNorm(encoder_layer.d_model)\n",
    "\n",
    "    def forward(self, src_embeddings_batch, src_mask):\n",
    "        src_representations_batch = src_embeddings_batch\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            src_representations_batch = encoder_layer(src_representations_batch, src_mask)\n",
    "        return self.LN(src_representations_batch) # Using LN. not mentioned explicitly in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79650ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, decoder_layer, number_of_layers):\n",
    "        super().__init__()\n",
    "        self.decoder_layers = replicate_module(decoder_layer, number_of_layers)\n",
    "        self.LN = nn.LayerNorm(decoder_layer.d_model)\n",
    "\n",
    "    def forward(self, trg_embeddings_batch, src_representations_batch, trg_mask, src_mask):\n",
    "        trg_representations_batch = trg_embeddings_batch\n",
    "\n",
    "        # Forward pass through the decoder stack\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            trg_representations_batch = decoder_layer(trg_representations_batch, src_representations_batch,\n",
    "                                                      trg_mask, src_mask)\n",
    "        return self.LN(trg_representations_batch)  # Using LN. not mentioned explicitly in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabd0aa5",
   "metadata": {},
   "source": [
    "# Part3: ================ transformer ================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "674060a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, src_vocab_size, trg_vocab_size, heads, number_of_layers, p_prob):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embeds source/target token ids into embedding vectors\n",
    "        self.src_embedding = Embedding(src_vocab_size, d_model)\n",
    "        self.trg_embedding = Embedding(trg_vocab_size, d_model)\n",
    "\n",
    "        # Adds positional information to source/target token's embedding vector\n",
    "        # (otherwise we'd lose the positional information which is important in human languages)\n",
    "        self.src_pos_embedding = PositionalEncoding(d_model, p_prob)\n",
    "        self.trg_pos_embedding = PositionalEncoding(d_model, p_prob)\n",
    "\n",
    "        encoder_layer = EncoderLayer(d_model, heads, p_prob)\n",
    "        decoder_layer = DecoderLayer(d_model, heads, p_prob)\n",
    "\n",
    "        self.encoder = Encoder(encoder_layer, number_of_layers)\n",
    "        self.decoder = Decoder(decoder_layer, number_of_layers)\n",
    "\n",
    "        # Converts final target token representations into log probabilities vectors of the target vocab size\n",
    "        self.generator = Generator(d_model, trg_vocab_size)\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self, default_initialization=False):\n",
    "        if not default_initialization:\n",
    "            for name, p in self.named_parameters():\n",
    "                if p.dim() > 1:\n",
    "                    nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, src_token_ids_batch, trg_token_ids_batch, src_mask, trg_mask):\n",
    "        src_representations_batch = self.encode(src_token_ids_batch, src_mask)\n",
    "        trg_log_probs = self.decode(trg_token_ids_batch, src_representations_batch, trg_mask, src_mask)\n",
    "        return trg_log_probs\n",
    "\n",
    "    def encode(self, src_token_ids_batch, src_mask):\n",
    "        src_embeddings_batch = self.src_embedding(src_token_ids_batch)  # (bs, seq_len) -> (bs, seq_len, d_model)\n",
    "        src_embeddings_batch = self.src_pos_embedding(src_embeddings_batch)\n",
    "        src_representations_batch = self.encoder(src_embeddings_batch, src_mask)\n",
    "\n",
    "        return src_representations_batch\n",
    "\n",
    "    def decode(self, trg_token_ids_batch, src_representations_batch, trg_mask, src_mask):\n",
    "        trg_embeddings_batch = self.trg_embedding(trg_token_ids_batch)  # (bs, seq_len) -> (bs, seq_len, d_model)\n",
    "        trg_embeddings_batch = self.trg_pos_embedding(trg_embeddings_batch)\n",
    "        trg_representations_batch = self.decoder(trg_embeddings_batch, src_representations_batch, trg_mask, src_mask)\n",
    "\n",
    "        # linear projection followed by log softmax\n",
    "        trg_log_probs = self.generator(trg_representations_batch) # (bs, seq_len, d_model) -> (bs, seq_len, vocab_size)\n",
    "\n",
    "        # (bs*seq_len, vocab_size) format for passing it into KL div loss\n",
    "        trg_log_probs = trg_log_probs.reshape(-1, trg_log_probs.shape[-1]) # (bs, seq_len, vocab_size) -> (bs*seq_len, vocab_size)\n",
    "\n",
    "        return trg_log_probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ac86db",
   "metadata": {},
   "source": [
    "# Part4: ================= tests ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "241b8b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_print(foo, interval=2):\n",
    "  def func(*args, **kwargs):\n",
    "    result = foo(*args,**kwargs)\n",
    "    time.sleep(interval)\n",
    "    print(\"=+\"*30, \"\\n\")\n",
    "    return result\n",
    "  return func\n",
    "\n",
    "@time_print\n",
    "def test_positional_encoding():\n",
    "    bs = 4\n",
    "    seq_len = 2048\n",
    "    d_model = 4\n",
    "    embeddings_batch = torch.zeros(bs, seq_len, d_model)\n",
    "    # print(\"Test input embedding batch data:\\n\", embeddings_batch)\n",
    "    pe = PositionalEncoding(d_model,  max_seq_length=20000)\n",
    "    output = pe(embeddings_batch)\n",
    "    print(\"Test positional_encoding. PE value:\\n\", output)\n",
    "    \n",
    "\n",
    "@time_print\n",
    "def test_multi_head_attention():\n",
    "    bs = 4\n",
    "    seq_len = 2048\n",
    "    d_model = 512\n",
    "    test_qkv = torch.ones(bs, seq_len, d_model)\n",
    "    print(\"input test data batch: \\n\", test_qkv)\n",
    "    multi_headed_attention = MultiHeadedAttention(d_model=d_model, heads=8)\n",
    "    output = multi_headed_attention(test_qkv, test_qkv, test_qkv, None)\n",
    "    print(\"output data batch: \\n\", output)\n",
    "    print(f\"Test multi_head_attention. Input shape:{(bs, seq_len, d_model)} Output shape: {output.shape}\")\n",
    "    assert output.shape == (bs, seq_len, d_model)\n",
    "\n",
    "\n",
    "@time_print\n",
    "def test_transformer():\n",
    "    vocab_size = 1024\n",
    "    batch_size = 4\n",
    "    seq_length = 100\n",
    "    transformer = Transformer(d_model=512, src_vocab_size=vocab_size, trg_vocab_size=vocab_size,\n",
    "                              heads=8, number_of_layers=2, p_prob=0.3)\n",
    "    src_token_ids_batch = torch.randint(0, 1000, size=(batch_size, seq_length))\n",
    "    trg_token_ids_batch = torch.randint(0, 1000, size=(batch_size, seq_length))\n",
    "    out = transformer(src_token_ids_batch, trg_token_ids_batch, src_mask=None, trg_mask=None)\n",
    "    print(f\"Test transformer. Input shape: {(batch_size, seq_length)} Output shape: {out.shape}\")\n",
    "    assert out.shape == (batch_size*seq_length, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "96ea009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RPF_count_file = '/home/user/data3/rbase/translation_pred/models/test/SRR15513158.read_count.pkl'\n",
    "tx_seq_file = '/home/user/data3/rbase/translation_pred/models/lib/tx_seq.v48.pkl'\n",
    "with open(RPF_count_file, 'rb') as f:\n",
    "        RPF_counts = pickle.load(f)\n",
    "with open(tx_seq_file, 'rb') as f:\n",
    "        tx_seq = pickle.load(f)\n",
    "d_model = 4\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0c00221a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GTGGAGCGCTGCGGAGGGTGCGTGCGGGCCGCGGCAGCCGAACAAAGGAGCAGGGGCGCCGCCGCAGGGACCCGCCACCCACCTCCCGGGGCCGCGCAGCGGCCTCTCGTCTACTGCCACCATGACCGCCAACGGCACAGCCGAGGCGGTGCAGATCCAGTTCGGCCTCATCAACTGCGGCAACAAGTACCTGACGGCCGAGGCGTTCGGGTTCAAGGTGAACGCGTCCGCCAGCAGCCTGAAGAAGAAGCAGATCTGGACGCTGGAGCAGCCCCCTGACGAGGCGGGCAGCGCGGCCGTGTGCCTGCGCAGCCACCTGGGCCGCTACCTGGCGGCGGACAAGGACGGCAACGTGACCTGCGAGCGCGAGGTGCCCGGTCCCGACTGCCGTTTCCTCATCGTGGCGCACGACGACGGTCGCTGGTCGCTGCAGTCCGAGGCGCACCGGCGCTACTTCGGCGGCACCGAGGACCGCCTGTCCTGCTTCGCGCAGACGGTGTCCCCCGCCGAGAAGTGGAGCGTGCACATCGCCATGCACCCTCAGGTCAACATCTACAGCGTCACCCGTAAGCGCTACGCGCACCTGAGCGCGCGGCCGGCCGACGAGATCGCCGTGGACCGCGACGTGCCCTGGGGCGTCGACTCGCTCATCACCCTCGCCTTCCAGGACCAGCGCTACAGCGTGCAGACCGCCGACCACCGCTTCCTGCGCCACGACGGGCGCCTGGTGGCGCGCCCCGAGCCGGCCACTGGCTACACGCTGGAGTTCCGCTCCGGCAAGGTGGCCTTCCGCGACTGCGAGGGCCGTTACCTGGCGCCGTCGGGGCCCAGCGGCACGCTCAAGGCGGGCAAGGCCACCAAGGTGGGCAAGGACGAGCTCTTTGCTCTGGAGCAGAGCTGCGCCCAGGTCGTGCTGCAGGCGGCCAACGAGAGGAACGTGTCCACGCGCCAGGGTATGGACCTGTCTGCCAATCAGGACGAGGAGACCGACCAGGAGACCTTCCAGCTGGAGATCGACCGCGACACCAAAAAGTGTGCCTTCCGTACCCACACGGGCAAGTACTGGACGCTGACGGCCACCGGGGGCGTGCAGTCCACCGCCTCCAGCAAGAATGCCAGCTGCTACTTTGACATCGAGTGGCGTGACCGGCGCATCACACTGAGGGCGTCCAATGGCAAGTTTGTGACCTCCAAGAAGAATGGGCAGCTGGCCGCCTCGGTGGAGACAGCAGGGGACTCAGAGCTCTTCCTCATGAAGCTCATCAACCGCCCCATCATCGTGTTCCGCGGGGAGCATGGCTTCATCGGCTGCCGCAAGGTCACGGGCACCCTGGACGCCAACCGCTCCAGCTATGACGTCTTCCAGCTGGAGTTCAACGATGGCGCCTACAACATCAAAGACTCCACAGGCAAATACTGGACGGTGGGCAGTGACTCCGCGGTCACCAGCAGCGGCGACACTCCTGTGGACTTCTTCTTCGAGTTCTGCGACTATAACAAGGTGGCCATCAAGGTGGGCGGGCGCTACCTGAAGGGCGACCACGCAGGCGTCCTGAAGGCCTCGGCGGAAACCGTGGACCCCGCCTCGCTCTGGGAGTACTAGGGCCGGCCCGTCCTTCCCCGCCCCTGCCCACATGGCGGCTCCTGCCAACCCTCCCTGCTAACCCCTTCTCCGCCAGGTGGGCTCCAGGGCGGGAGGCAAGCCCCCTTGCCTTTCAAACTGGAAACCCCAGAGAAAACGGTGCCCCCACCTGTCGCCCCTATGGACTCCCCACTCTCCCCTCCGCCCGGGTTCCCTACTCCCCTCGGGTCAGCGGCTGCGGCCTGGCCCTGGGAGGGATTTCAGATGCCCCTGCCCTCTTGTCTGCCACGGGGCGAGTCTGGCACCTCTTTCTTCTGACCTCAGACGGCTCTGAGCCTTATTTCTCTGGAAGCGGCTAAGGGACGGTTGGGGGCTGGGAGCCCTGGGCGTGTAGTGTAACTGGAATCTTTTGCCTCTCCCAGCCACCTCCTCCCAGCCCCCCAGGAGAGCTGGGCACATGTCCCAAGCCTGTCAGTGGCCCTCCCTGGTGCACTGTCCCCGAAACCCCTGCTTGGGAAGGGAAGCTGTCGGGTGGGCTAGGACTGACCCTTGTGGTGTTTTTTTGGGTGGTGGCTGGAAACAGCCCCTCTCCCACGTGGCAGAGGCTCAGCCTGGCTCCCTTCCCTGGAGCGGCAGGGCGTGACGGCCACAGGGTCTGCCCGCTGCACGTTCTGCCAAGGTGGTGGTGGCGGGCGGGTAGGGGTGTGGGGGCCGTCTTCCTCCTGTCTCTTTCCTTTCACCCTAGCCTGACTGGAAGCAGAAAATGACCAAATCAGTATTTTTTTTAATGAAATATTATTGCTGGAGGCGTCCCAGGCAAGCCTGGCTGTAGTAGCGAGTGATCTGGCGGGGGGCGTCTCAGCACCCTCCCCAGGGGGTGCATCTCAGCCCCCTCTTTCCGTCCTTCCCGTCCAGCCCCAGCCCTGGGCCTGGGCTGCCGACACCTGGGCCAGAGCCCCTGCTGTGATTGGTGCTCCCTGGGCCTCCCGGGTGGATGAAGCCAGGCGTCGCCCCCTCCGGGAGCCCTGGGGTGAGCCGCCGGGGCCCCCCTGCTGCCAGCCTCCCCCGTCCCCAACATGCATCTCACTCTGGGTGTCTTGGTCTTTTATTTTTTGTAAGTGTCATTTGTATAACTCTAAACGCCCATGATAGTAGCTTCAAACTGGAAATAGCGAAATAAAATAACTCAGTCTGCAGCCCCA\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "RNASequenceDataset.one_hot_encode() takes from 1 to 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(tx_seq[\u001b[33m'\u001b[39m\u001b[33mENST00000382361.8\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m dataset = \u001b[43mRNASequenceDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(dataset.len())\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(dataset.getitem(\u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/user/data3/rbase/translation_pred/models/src/dataset_prepare.py:54\u001b[39m, in \u001b[36mRNASequenceDataset.__init__\u001b[39m\u001b[34m(self, transcript_seqs, batch_size, min_length, max_length, padding_value)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28mself\u001b[39m.padding_value = padding_value\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# 预计算批次索引\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[38;5;28mself\u001b[39m.batch_indices = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_precompute_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/user/data3/rbase/translation_pred/models/src/dataset_prepare.py:81\u001b[39m, in \u001b[36mRNASequenceDataset._precompute_batches\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     79\u001b[39m     idx += \u001b[38;5;28mself\u001b[39m.batch_size\n\u001b[32m     80\u001b[39m     \u001b[38;5;66;03m# one-hot encode\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     embedings = np.array([\u001b[38;5;28mself\u001b[39m.one_hot_encode(\u001b[38;5;28mself\u001b[39m.seq_dict[tid], L) \u001b[38;5;28;01mfor\u001b[39;00m tid \u001b[38;5;129;01min\u001b[39;00m batch_items])\n\u001b[32m     82\u001b[39m     batches.append({\u001b[33m'\u001b[39m\u001b[33mtids\u001b[39m\u001b[33m'\u001b[39m: batch_items, \u001b[33m'\u001b[39m\u001b[33mlength\u001b[39m\u001b[33m'\u001b[39m: L, \u001b[33m'\u001b[39m\u001b[33membedings\u001b[39m\u001b[33m'\u001b[39m: embedings})\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# 剩余不足 batch_size 的序列，留到下一轮\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/user/data3/rbase/translation_pred/models/src/dataset_prepare.py:81\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     79\u001b[39m     idx += \u001b[38;5;28mself\u001b[39m.batch_size\n\u001b[32m     80\u001b[39m     \u001b[38;5;66;03m# one-hot encode\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     embedings = np.array([\u001b[38;5;28mself\u001b[39m.one_hot_encode(\u001b[38;5;28mself\u001b[39m.seq_dict[tid], L) \u001b[38;5;28;01mfor\u001b[39;00m tid \u001b[38;5;129;01min\u001b[39;00m batch_items])\n\u001b[32m     82\u001b[39m     batches.append({\u001b[33m'\u001b[39m\u001b[33mtids\u001b[39m\u001b[33m'\u001b[39m: batch_items, \u001b[33m'\u001b[39m\u001b[33mlength\u001b[39m\u001b[33m'\u001b[39m: L, \u001b[33m'\u001b[39m\u001b[33membedings\u001b[39m\u001b[33m'\u001b[39m: embedings})\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# 剩余不足 batch_size 的序列，留到下一轮\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: RNASequenceDataset.one_hot_encode() takes from 1 to 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "print(tx_seq['ENST00000382361.8'])\n",
    "dataset = RNASequenceDataset(tx_seq, batch_size, min_length=100)\n",
    "max_batch_idx = dataset.len() - 1\n",
    "    print(dataset.get_batch(max_batch_idx))\n",
    "    print(dataset.get_batch_embedding_shape(max_batch_idx))\n",
    "    print(dataset.get_batch_embedding(max_batch_idx, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4242f387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test positional_encoding. PE value:\n",
      " tensor([[[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
      "         [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
      "         [ 0.9093, -0.4161,  0.0200,  0.9998],\n",
      "         ...,\n",
      "         [ 0.1759, -0.9844,  0.9996, -0.0296],\n",
      "         [-0.7333, -0.6799,  0.9992, -0.0396],\n",
      "         [-0.9683,  0.2497,  0.9988, -0.0496]],\n",
      "\n",
      "        [[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
      "         [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
      "         [ 0.9093, -0.4161,  0.0200,  0.9998],\n",
      "         ...,\n",
      "         [ 0.1759, -0.9844,  0.9996, -0.0296],\n",
      "         [-0.7333, -0.6799,  0.9992, -0.0396],\n",
      "         [-0.9683,  0.2497,  0.9988, -0.0496]],\n",
      "\n",
      "        [[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
      "         [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
      "         [ 0.9093, -0.4161,  0.0200,  0.9998],\n",
      "         ...,\n",
      "         [ 0.1759, -0.9844,  0.9996, -0.0296],\n",
      "         [-0.7333, -0.6799,  0.9992, -0.0396],\n",
      "         [-0.9683,  0.2497,  0.9988, -0.0496]],\n",
      "\n",
      "        [[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
      "         [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
      "         [ 0.9093, -0.4161,  0.0200,  0.9998],\n",
      "         ...,\n",
      "         [ 0.1759, -0.9844,  0.9996, -0.0296],\n",
      "         [-0.7333, -0.6799,  0.9992, -0.0396],\n",
      "         [-0.9683,  0.2497,  0.9988, -0.0496]]])\n",
      "=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+ \n",
      "\n",
      "input test data batch: \n",
      " tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])\n",
      "output data batch: \n",
      " tensor([[[ 0.1010, -0.0605,  0.0425,  ...,  0.0978, -0.1662,  0.3673],\n",
      "         [ 0.1010, -0.0605,  0.0425,  ...,  0.0978, -0.1662,  0.3673],\n",
      "         [ 0.1010, -0.0605,  0.0425,  ...,  0.0978, -0.1662,  0.3673],\n",
      "         ...,\n",
      "         [ 0.1010, -0.0605,  0.0425,  ...,  0.0978, -0.1662,  0.3673],\n",
      "         [ 0.1010, -0.0605,  0.0425,  ...,  0.0978, -0.1662,  0.3673],\n",
      "         [ 0.1010, -0.0605,  0.0425,  ...,  0.0978, -0.1662,  0.3673]],\n",
      "\n",
      "        [[ 0.1010, -0.0605,  0.0425,  ...,  0.0978, -0.1662,  0.3673],\n",
      "         [ 0.1010, -0.0605,  0.0425,  ...,  0.0978, -0.1662,  0.3673],\n",
      "         [ 0.1010, -0.0605,  0.0425,  ...,  0.0978, -0.1662,  0.3673],\n",
      "         ...,\n",
      "         [ 0.1010, -0.0605,  0.0425,  ...,  0.0978, -0.1662,  0.3673],\n",
      "         [ 0.1010, -0.0605,  0.0425,  ...,  0.0978, -0.1662,  0.3673],\n",
      "         [ 0.1010, -0.0605,  0.0425,  ...,  0.0978, -0.1662,  0.3673]],\n",
      "\n",
      "        [[ 0.1010, -0.0605,  0.0425,  ...,  0.0978, -0.1662,  0.3673],\n",
      "         [ 0.1010, -0.0605,  0.0425,  ...,  0.0978, -0.1662,  0.3673],\n",
      "         [ 0.1010, -0.0605,  0.0425,  ...,  0.0978, -0.1662,  0.3673],\n",
      "         ...,\n",
      "         [ 0.1010, -0.0605,  0.0425,  ...,  0.0978, -0.1662,  0.3673],\n",
      "         [ 0.1010, -0.0605,  0.0425,  ...,  0.0978, -0.1662,  0.3673],\n",
      "         [ 0.1010, -0.0605,  0.0425,  ...,  0.0978, -0.1662,  0.3673]],\n",
      "\n",
      "        [[ 0.1010, -0.0605,  0.0425,  ...,  0.0978, -0.1662,  0.3673],\n",
      "         [ 0.1010, -0.0605,  0.0425,  ...,  0.0978, -0.1662,  0.3673],\n",
      "         [ 0.1010, -0.0605,  0.0425,  ...,  0.0978, -0.1662,  0.3673],\n",
      "         ...,\n",
      "         [ 0.1010, -0.0605,  0.0425,  ...,  0.0978, -0.1662,  0.3673],\n",
      "         [ 0.1010, -0.0605,  0.0425,  ...,  0.0978, -0.1662,  0.3673],\n",
      "         [ 0.1010, -0.0605,  0.0425,  ...,  0.0978, -0.1662,  0.3673]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Test multi_head_attention. Input shape:(4, 2048, 512) Output shape: torch.Size([4, 2048, 512])\n",
      "=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+ \n",
      "\n",
      "Test transformer. Input shape: (4, 100) Output shape: torch.Size([400, 1024])\n",
      "=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    test_positional_encoding()\n",
    "    test_multi_head_attention()\n",
    "    test_transformer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ribomodel",
   "language": "python",
   "name": "ribomodel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
